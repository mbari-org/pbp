{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Integrating pyhydrophone in the processing pipeline\n",
   "id": "d087f632"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Some parameters for PBP",
   "id": "6fe8dc4d-97c2-4e40-a4f7-845d7e716855"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will process the data from station MB05, which was recorded with a SoundTrap, and we'll use pyhydrophone to obtain the calibration information",
   "id": "df169584c0ea92a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's start importing the packages we'll need",
   "id": "a8e992d093e4c739"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Import package modules\n",
    "import xarray as xr\n",
    "import dask\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "import pyhydrophone as pyhy\n",
    "from google.cloud.storage import Client as GsClient  # To handle download of `gs:` resources\n",
    "\n",
    "\n",
    "from pbp.meta_gen.gen_soundtrap import SoundTrapMetadataGenerator\n",
    "from pbp.logging_helper import create_logger_info, create_logger\n",
    "\n",
    "from pbp.process_helper import ProcessHelper\n",
    "from pbp.file_helper import FileHelper"
   ],
   "id": "3c293bf0c5bced93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And describe where are the data stored",
   "id": "459a6a62a346f7f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Audio data input specifications\n",
    "wav_uri              = 's3://pacific-sound-mb05'   # cloud storage location for the input audio data\n",
    "start_date           = '20220922'   # start date for temporal metadata extraction (YYYYMMDD)\n",
    "end_date             = '20220925'   # end date for temporal metadata extraction (YYYYMMDD)\n",
    "json_base_dir        = 'metadata/json' # location to store generated metadata in JSON format\n",
    "xml_dir              = 'metadata/xml' # location to store downloaded xml files from the soundtrap\n",
    "\n",
    "# Output data specifications\n",
    "download_dir        = 'downloads'\n",
    "output_dir          = 'hmd_output'\n",
    "output_prefix       = 'MB05_'\n",
    "\n",
    "# ST information \n",
    "serial_number        = 6715      # ST serial number\n",
    "st_model             = 'SoundTrap 600 HF'  # Needs to match the right model in the ST calibration website: https://oceaninstruments.azurewebsites.net\n",
    "st_gain              = 'High'      # Can be 'High' or 'Low'\n",
    "subset_to            = (10, 2_000)  # min, max frequencies to subset to \n",
    "\n",
    "# Information of the metadata which will be included in the netcdf files\n",
    "global_attrs_uri    = 'metadata/mb05/globalAttributes_NRS11.yaml'\n",
    "variable_attrs_uri  = 'metadata/mb05/variableAttributes_NRS11.yaml'"
   ],
   "id": "cea6ee503b469771"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " # a logger that only logs messages tagged as info to the console, for more verbose logging\n",
    "log = create_logger_info(f'soundtrap_{start_date}_{end_date}')\n",
    "\n",
    "# Convert the start and end dates to datetime objects\n",
    "start = datetime.strptime(start_date, \"%Y%m%d\")\n",
    "end = datetime.strptime(end_date, \"%Y%m%d\")\n",
    "\n",
    "# Create the metadata generator\n",
    "meta_gen = SoundTrapMetadataGenerator(\n",
    "        log=log,\n",
    "        uri=wav_uri,\n",
    "        json_base_dir=json_base_dir,\n",
    "        xml_dir=xml_dir,\n",
    "        start=start,\n",
    "        end=end,\n",
    "        prefixes=[str(serial_number)])\n",
    "\n",
    "# Generate the metadata - this will generate JSON files in the json_base_dir\n",
    "meta_gen.run()"
   ],
   "id": "ab590c7ec3809cee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "st = pyhy.SoundTrap(model=st_model,\n",
    "                    serial_number=serial_number,\n",
    "                    name=f'{st_model}_{serial_number}',\n",
    "                    gain_type=st_gain)\n",
    "print('SoundTrap settings to:')\n",
    "print('sensitivity: ', st.sensitivity)\n",
    "print('Vpp: ', st.Vpp)\n",
    "print('preamp_gain: ', st.preamp_gain)\n",
    "print('gain_type: ', 'High')"
   ],
   "id": "e6d1961529a33b7e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Supporting functions\n",
    "\n",
    "PBP includes these two main modules that we will be using below:\n",
    "\n",
    "- `FileHelper`: Facilitates input file reading. It supports reading local files as well as from GCP (`gs://` URIs) and AWS (`s3://` URIs).\n",
    "- `ProcessHelper`: The main processing module.\n",
    "\n",
    "We first define a function that takes care of HMB generation for a given date.\n",
    "\n",
    "Based on that function, we then define one other function to dispatch multiple dates in parallel.\n"
   ],
   "id": "bda41f011908aec9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## A function to process a given day\n",
    "\n",
    "Supported by those PBP modules, we define a function that takes care of processing a given day:"
   ],
   "id": "dd0550d764fc08db"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def process_date(date: str, gen_netcdf: bool = True):\n",
    "    \"\"\"\n",
    "    Main function to generate the HMB product for a given day.\n",
    "\n",
    "    It makes use of supporting elements in PBP in terms of logging,\n",
    "    file handling, and PyPAM based HMB generation.\n",
    "\n",
    "    :param date: Date to process, in YYYYMMDD format.\n",
    "\n",
    "    :param gen_netcdf:  Allows caller to skip the `.nc` creation here\n",
    "    and instead save the datasets after all days have been generated\n",
    "    (see parallel execution below).\n",
    "\n",
    "    :return: the generated xarray dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    log_filename = f\"{output_dir}/{output_prefix}{date}.log\"\n",
    "\n",
    "    logger = create_logger(\n",
    "        log_filename_and_level=(log_filename, logging.INFO),\n",
    "        console_level=None,\n",
    "    )\n",
    "\n",
    "    # we are only downloading publicly accessible datasets:\n",
    "    gs_client = GsClient.create_anonymous_client()\n",
    "\n",
    "    file_helper = FileHelper(\n",
    "        logger=logger,\n",
    "        json_base_dir=json_base_dir,\n",
    "        gs_client=gs_client,\n",
    "        download_dir=download_dir,\n",
    "        assume_downloaded_files=True,\n",
    "        retain_downloaded_files=True,\n",
    "    )\n",
    "    \n",
    "    process_helper = ProcessHelper(\n",
    "        log=log,\n",
    "        file_helper=file_helper,\n",
    "        output_dir=output_dir,\n",
    "        output_prefix=output_prefix,\n",
    "        global_attrs_uri=global_attrs_uri,\n",
    "        variable_attrs_uri=variable_attrs_uri,\n",
    "        voltage_multiplier=st.Vpp/2,   # For pyhydrophone, Vpp is the voltage peak-to-peak, while for pbp voltage multiplier is 0 to peak\n",
    "        sensitivity_uri=None,\n",
    "        sensitivity_flat_value=-st.sensitivity,  # Please note the minus (-) signal\n",
    "        subset_to=subset_to,\n",
    "    )\n",
    "\n",
    "    ## now, get the HMB result:\n",
    "    print(f'::: Started processing {date=}    {log_filename=}')\n",
    "    result = process_helper.process_day(date)\n",
    "\n",
    "    if gen_netcdf:\n",
    "        nc_filename = f\"{output_dir}/{output_prefix}{date}.nc\"\n",
    "        print(f':::   Ended processing {date=} =>  {nc_filename=}')\n",
    "    else:\n",
    "        print(f':::   Ended processing {date=} => (dataset generated in memory)')\n",
    "\n",
    "    if result is not None:\n",
    "        return result.dataset\n",
    "    else:\n",
    "        print(f'::: UNEXPECTED: no segments were processed for {date=}')"
   ],
   "id": "ba002af85aa88554",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## A function to process multiple days\n",
    "\n",
    "We use [Dask](https://examples.dask.org/delayed.html) to dispatch, in parallel, multiple instances of the `process_date` function defined above."
   ],
   "id": "6acb2f03a74dece4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 6,
   "source": [
    "def process_multiple_dates(dates: list[str], gen_netcdf: bool = False) -> list[xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Generates HMB for multiple days in parallel using Dask.\n",
    "    Returns the resulting HMB datasets.\n",
    "    \n",
    "    :param dates: The dates to process, each in YYYYMMDD format.\n",
    "\n",
    "    :param gen_netcdf:  Allows caller to skip the `.nc` creation here\n",
    "    and instead save the datasets after all days have been generated.\n",
    "\n",
    "    :return: the list of generated datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    @dask.delayed\n",
    "    def delayed_process_date(date: str):\n",
    "        return process_date(date, gen_netcdf=gen_netcdf)\n",
    "    \n",
    "    ## To display total elapsed time at the end the processing:\n",
    "    start_time = time.time()\n",
    "\n",
    "    ## This will be called by Dask when all dates have completed processing:\n",
    "    def aggregate(*datasets) -> list[xr.Dataset]:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f'===> All {len(datasets)} dates completed. Elapsed time: {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} mins)')\n",
    "        return datasets\n",
    "\n",
    "\n",
    "    ## Prepare the processes:\n",
    "    delayed_processes = [delayed_process_date(date) for date in dates]\n",
    "    aggregation = dask.delayed(aggregate)(*delayed_processes)\n",
    "\n",
    "    ## And launch them:\n",
    "    return aggregation.compute()\n"
   ],
   "id": "207a236b32880dc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# In general, we can use pandas to help us generate the list of dates we want to process\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='1D')\n",
    "dates = date_range.strftime(\"%Y%m%d\").tolist()\n",
    "\n",
    "# Now, launch the generation:\n",
    "print(f\"Launching HMB generation for {len(dates)} {dates=}\")\n",
    "\n",
    "# Get all HMB datasets:\n",
    "generated_datasets = process_multiple_dates(dates, gen_netcdf=True)\n",
    "print(f\"Generated datasets: {len(generated_datasets)}\\n\")"
   ],
   "id": "cbda543629c949ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
